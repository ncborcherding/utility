---
title: "Integration"
output: html_document
date: "2024-03-20"
---

# Set Up

## Installing scvi-tools

```{r eval = FALSE}
library(reticulate)
conda_create("scvi-env")
conda_install("sci-env", "scvi-tools", pip = TRUE)
conda_install("sci-env", "scanpy", pip = TRUE)
```

## Loading Libraries and Functions

```{r}
library(Seurat)
library(SeuratWrappers)

source("./R/checkAndUpdateGenes.R")
```

## Finding Initial Features Space for Full Integration

Here I am randomly sampling 20% of the Seurat Objects in order to get the 4000 most variable features. This sampling is across tissues and tumors to make it robust. We will use the features to subset the Seurat Objects towards a full integration.

```{r}
files <- list.files("./data/processedData/seuratObjects/")
names <- stringr::str_remove_all(files, ".rds")

set.seed(42)
sampled <- sample(727, 146)
files <- files[sampled]
names <- names[sampled]
object.list <- lapply(seq_len(length(files)), function(x) {
                      obj <- readRDS(paste0("./data/processedData/seuratObjects/", files[x]))
                      
                      LayerData(obj, layer = "data") <- NULL
                      obj@meta.data <- obj[[]][,1:6]
                      obj$orig.ident <- names[x]
                      obj
})
gc()

object.merge <- merge(object.list[[1]], object.list[-1])
rm(object.list)
gc()

object.merge  <- NormalizeData(object.merge )
object.merge <- JoinLayers(object.merge)
object.merge  <- FindVariableFeatures(object.merge, nfeatures = 6000)

# Removing specific feature sets
features.to.use <- VariableFeatures(object.merge)
features.to.use <- Ibex::quietBCRgenes(features.to.use)
features.to.use <- Trex::quietTCRgenes(features.to.use)
features.to.use <- features.to.use[!grepl("RPS|RPL|LINC|MT-|AC0|AC1|AC2|RP11-|MIR|HB|IGHV", features.to.use)]

#Taking the top 4000 features for integration
new.features <- features.to.use[1:4000]

saveRDS(features.to.use, "./outputs/reference/Overall_featureSpace.rds")
rm(object.merge)
gc()
```

# Integrating All Cells with BPCells and scVI

Now we can take all the Seurat Objects and integrate them. The following code has been optimized for memory efficiency and scalability. It uses `BPCells` to store the count matrices on disk, avoiding the need to load the entire dataset into RAM. It then uses `scVI` for integration, which is more scalable than traditional methods like Harmony or MNN. The raw count matrices are first filtered and subset to a common feature space before being converted to the `BPCells` format.

```{r}
# This chunk refactors the integration process to be more memory-efficient
# and scalable. It uses BPCells to store data on-disk and scVI for integration.

# 1. Setup BPCells directory
# Create a directory to store the on-disk BPCells matrices
bp_dir <- "./data/processedData/bpcells_matrices/"
if (!dir.exists(bp_dir)) {
  dir.create(bp_dir, recursive = TRUE)
}

# 2. Load feature space and file list
files <- list.files("./data/processedData/seuratObjects/")
names <- stringr::str_remove_all(files, ".rds")
features.to.use <- readRDS("./outputs/reference/Overall_featureSpace.rds")

# 3. Process objects one-by-one and convert to BPCells format
# This loop reads each Seurat object, applies initial filtering, subsets to
# the common feature space, and then writes the count matrix to an on-disk
# BPCells matrix. This avoids loading all data into RAM at once.
object.list <- lapply(seq_len(length(files)), function(x) {
    cat("Processing:", files[x], "\n")
    obj <- readRDS(paste0("./data/processedData/seuratObjects/", files[x]))

    # Apply filtering strategy from the original script to reduce object size
    # Step 1: Only taking T/NK cells from Automated Annotation
    if("predicted.celltype.l1" %in% colnames(obj[[]])) {
        obj <- subset(obj, predicted.celltype.l1 %in% c("CD4 T", "CD8 T", "other T", "NK"))
    }
    # Step 2: Only cells with contig data
    if("CTaa" %in% colnames(obj[[]])) {
        cells <- which(!is.na(obj[[]][,"CTaa"]))
        obj <- subset(obj, cells = colnames(obj)[cells])
    }
    # Step 3: Only cells estimated to be singlets
    if("db.class" %in% colnames(obj[[]])) {
        obj <- subset(obj, db.class == "singlet")
    }

    # Intersect features to ensure no errors from missing features
    current_features <- intersect(features.to.use, rownames(obj))
    obj <- subset(obj, features = current_features)
    obj$orig.ident <- names[x]

    # Write the counts to a BPCells-backed matrix
    # This is the key step for memory efficiency
    bp_path <- file.path(bp_dir, names[x])
    BPCells::write_matrix_dir(
        mat = obj@assays$RNA@layers$counts,
        dir = bp_path
    )

    # Create a new Seurat object with the on-disk matrix
    obj_bp <- Seurat::CreateSeuratObject(
        counts = BPCells::open_matrix_dir(dir = bp_path),
        meta.data = obj[[]]
    )
    obj_bp@assays$RNA@layers$counts@dimnames[[1]] <- current_features

    # Clean up memory
    rm(obj)
    gc()

    return(obj_bp)
})
gc()

# 4. Integrate using scVI
# RunScviIntegration is a scalable integration method that replaces the
# memory-intensive `merge` and `RunHarmony` steps.
object.integrated <- SeuratWrappers::RunScviIntegration(
  object.list,
  assay = "RNA",
  reduction = "scvi",
  scvi_model_path = "./models/scvi/"
)
rm(object.list)
gc()

# The object is now integrated. The default assay is now `integrated`.
# We can proceed with downstream analysis like PCA and UMAP on the integrated data.
object.integrated <- RunPCA(object.integrated, npcs = 30, verbose = FALSE)
object.integrated <- RunUMAP(object.integrated,
                             reduction = "scvi",
                             dims = 1:30,
                             reduction.name = "umap.scvi")

# Add tissue metadata
object.integrated$tissue <- stringr::str_split(object.integrated$orig.ident, "[.]", simplify = TRUE)[,2]

# Save the final, memory-efficient, and well-integrated object
saveRDS(object.integrated, "./data/integratedSeuratObject_scVI.rds")

# 5. Visualize the results
# Visualize the UMAP, grouping by different metadata columns to assess integration
DimPlot(
  object.integrated,
  reduction = "umap.scvi",
  group.by = c("orig.ident", "tissue"),
  combine = FALSE, 
  label.size = 2
)

# Visualize expression of key marker genes on the new UMAP
FeaturePlot(object.integrated, c("CD4", "CD8A", "FOXP3"), reduction = "umap.scvi")

```

# Leiden Clustering and Evaluation

This section performs cell clustering using the Leiden algorithm, which is a fast and high-quality community detection algorithm. To determine the optimal clustering granularity, we systematically test a range of resolution parameters. The best resolution is selected by finding the one that maximizes the average silhouette width, a measure of cluster cohesion and separation.

```{r}
# Load required library for silhouette scoring
# Ensure the 'cluster' package is installed: install.packages('cluster')
library(cluster)

# 1. Find Neighbors
# This step builds a Shared Nearest Neighbor (SNN) graph, which is required for clustering.
# We use the 'scvi' reduction which represents the integrated data space.
object.integrated <- FindNeighbors(object.integrated, reduction = "scvi", dims = 1:30)

# 2. Evaluate Clustering Resolutions
# We will test a range of resolutions for the Leiden algorithm to find the one
# that gives the most coherent clusters. We'll use the average silhouette
# width as our metric. A higher silhouette score indicates better-defined clusters.

# Define the range of resolutions to test
resolutions <- seq(0.1, 1.2, by = 0.1)
silhouette_scores <- c()

for (res in resolutions) {
    cat("Testing resolution:", res, "\n")
    # Run Leiden clustering
    object.integrated <- FindClusters(object.integrated, resolution = res, algorithm.name = "leiden", verbose = FALSE)

    # Calculate silhouette score
    # Note: This can be computationally intensive on large datasets.
    # We take a random sample of 5000 cells to make it feasible.
    if (ncol(object.integrated) > 5000) {
      cells_for_sil <- sample(colnames(object.integrated), 5000)
    } else {
      cells_for_sil <- colnames(object.integrated)
    }

    # The silhouette function requires numeric cluster labels and a distance matrix
    sil <- silhouette(
        x = as.numeric(as.character(object.integrated@meta.data[cells_for_sil, "seurat_clusters"])),
        dist = dist(object.integrated@reductions$scvi@cell.embeddings[cells_for_sil, ])
    )

    # We use the summary of the silhouette object to get the average width
    avg_sil_width <- summary(sil)$avg.width
    silhouette_scores <- c(silhouette_scores, avg_sil_width)
}

# Plot the results to find the optimal resolution
plot(resolutions, silhouette_scores, type = "b", pch = 19,
     xlab = "Resolution", ylab = "Average Silhouette Width",
     main = "Leiden Clustering Resolution Optimization")

# 3. Final Clustering
# Choose the resolution that gives the highest silhouette score
optimal_resolution <- resolutions[which.max(silhouette_scores)]
cat("Optimal resolution found:", optimal_resolution, "\n")

# Re-run clustering with the optimal resolution
object.integrated <- FindClusters(object.integrated, resolution = optimal_resolution, algorithm.name = "leiden", verbose = FALSE)
# Rename the final clusters for clarity
object.integrated$leiden_clusters <- object.integrated$seurat_clusters

# 4. Visualize the Final Clustering
DimPlot(object.integrated, reduction = "umap.scvi", group.by = "leiden_clusters", label = TRUE)

# Save the final object with clustering information
saveRDS(object.integrated, "./data/integratedSeuratObject_scVI_clustered.rds")

```

 ####################
                      #Filtering Strategy
                      ###################
                      #Step 1: Only taking T/NK cells from Automated Annotation
                      if("predicted.celltype.l1" %in% colnames(obj[[]])) {
                        obj <- subset(obj, predicted.celltype.l1 %in% c("CD4 T", "CD8 T", "other T", "NK"))
                      } 
                      
                      #Step 2: Only cells with contig data
                      if("CTaa" %in% colnames(obj[[]])) {
                        cells <- which(!is.na(obj[[]][,"CTaa"]))
                        obj <- subset(obj, cells = colnames(obj)[cells])
                      }
                      
                      #Step 3: Only cells estimated to be singlets
                      if("db.class" %in% colnames(obj[[]])) {
                        obj <- subset(obj, db.class == "singlet")
                      }
